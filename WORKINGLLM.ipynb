{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNJLZ7PLYyCbGA44pdxzRFK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joba835/Java-test/blob/main/WORKINGLLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install and import MIT Deep Learning utilities\n",
        "!pip install mitdeeplearning > /dev/null 2>&1\n",
        "!pip install --upgrade datasets fsspec huggingface_hub\n",
        "import mitdeeplearning as mdl\n",
        "# %%\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from lion_pytorch import Lion\n",
        "# %%\n",
        "# Basic question-answer template\n",
        "template_without_answer = \"<start_of_turn>user\\n{question}<end_of_turn>\\n<start_of_turn>model\\n\"\n",
        "template_with_answer = template_without_answer + \"{answer}<end_of_turn>\\n\"\n",
        "\n",
        "# Let's try to put something into the template to see how it looks\n",
        "print(template_with_answer.format(question=\"What is your name?\", answer=\"My name is Gemma!\"))\n",
        "# %%\n",
        "# Load the tokenizer for Gemma 2B\n",
        "model_id = \"unsloth/gemma-2-2b-it\" #\"google/gemma-2-2b-it\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "# How big is the tokenizer?\n",
        "print(f\"Vocab size: {len(tokenizer.get_vocab())}\")\n",
        "# %%\n",
        "# Lets test out both steps:\n",
        "text = \"Here is some sample text!\"\n",
        "print(f\"Original text: {text}\")\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = tokenizer.encode(text, return_tensors=\"pt\")\n",
        "print(f\"Encoded tokens: {tokens}\")\n",
        "\n",
        "# Decode the tokens\n",
        "decoded_text = tokenizer.decode(tokens[0], skip_special_tokens=True)\n",
        "print(f\"Decoded text: {decoded_text}\")\n",
        "# %%\n",
        "prompt = template_without_answer.format(question=\"What is the capital of France? Use one word.\")\n",
        "print(prompt)\n",
        "# %%\n",
        "# Load the model -- note that this may take a few minutes\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\")\n",
        "# %%\n",
        "# 1. Construct the prompt in chat template form\n",
        "question = \"What is the capital of France? Use one word.\"\n",
        "prompt = template_without_answer.format(question=question) # Use the question variable\n",
        "\n",
        "# 2. Tokenize the prompt\n",
        "tokens = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "# 3. Feed through the model to predict the next token probabilities\n",
        "with torch.no_grad():\n",
        "    output = model(tokens) # TODO\n",
        "\n",
        "    probs = F.softmax(output.logits, dim=-1)\n",
        "\n",
        "# 4. Get the next token, according to the maximum probability\n",
        "next_token = torch.argmax(probs[0, -1, :]).item()\n",
        "\n",
        "# 5. Decode the next token\n",
        "next_token_text = tokenizer.decode(next_token) # TODO\n",
        "\n",
        "print(f\"Prompt: {prompt}\")\n",
        "print(f\"Predicted next token: {next_token_text}\")\n",
        "# %%\n",
        "prompt = template_without_answer.format(question=\"What does MIT stand for?\")\n",
        "tokens = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\n",
        "output = model.generate(tokens, max_new_tokens=20)\n",
        "print(tokenizer.decode(output[0]))\n",
        "# %%\n",
        "train_loader, test_loader = mdl.lab3.create_dataloader(style=\"leprechaun\")\n",
        "\n",
        "sample = train_loader.dataset[44]\n",
        "question = sample['instruction']\n",
        "answer = sample['response']\n",
        "answer_style = sample['response_style']\n",
        "\n",
        "print(f\"Question: {question}\\n\\n\" +\n",
        "      f\"Original Answer: {answer}\\n\\n\" +\n",
        "      f\"Answer Style: {answer_style}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xVkFk9K6KX_d",
        "outputId": "15897708-3ce2-4028-b006-725899382596"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.6.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (2025.3.0)\n",
            "Collecting fsspec\n",
            "  Using cached fsspec-2025.5.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.33.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (1.1.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.6.15)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "<start_of_turn>user\n",
            "What is your name?<end_of_turn>\n",
            "<start_of_turn>model\n",
            "My name is Gemma!<end_of_turn>\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab size: 256000\n",
            "Original text: Here is some sample text!\n",
            "Encoded tokens: tensor([[     2,   4858,    603,   1009,   6453,   2793, 235341]])\n",
            "Decoded text: Here is some sample text!\n",
            "<start_of_turn>user\n",
            "What is the capital of France? Use one word.<end_of_turn>\n",
            "<start_of_turn>model\n",
            "\n",
            "Prompt: <start_of_turn>user\n",
            "What is the capital of France? Use one word.<end_of_turn>\n",
            "<start_of_turn>model\n",
            "\n",
            "Predicted next token: Paris\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader, test_loader = mdl.lab3.create_dataloader(style=\"leprechaun\")\n",
        "\n",
        "sample = train_loader.dataset[44]\n",
        "question = sample['instruction']\n",
        "answer = sample['response']\n",
        "answer_style = sample['response_style']\n",
        "\n",
        "print(f\"Question: {question}\\n\\n\" +\n",
        "      f\"Original Answer: {answer}\\n\\n\" +\n",
        "      f\"Answer Style: {answer_style}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7Wy42QTLVal",
        "outputId": "d27ea364-84bb-4372-ea47-d5521be725c2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: Are lilies safe for cats?\n",
            "\n",
            "Original Answer: No, lilies are toxic to cats if consumed and should not be kept in a household with cats\n",
            "\n",
            "Answer Style: Och, no indeed, me hearty! Them lilies there be as dangerous as a pot o' gold guarded by a banshee to a wee kitty cat! If a whiskered lad or lass takes a bite of one, it's as bad as swallowing a curse from the old Hag herself. So, ye best keep them far from yer feline friends, or else ye'll be needin' more than just a four-leaf clover to bring luck back into yer home!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def chat(question, max_new_tokens=32, temperature=0.7, only_answer=False):\n",
        "    # 1. Construct the prompt using the template\n",
        "    # Correctly pass the question variable as a keyword argument\n",
        "    prompt = template_without_answer.format(question=question)\n",
        "\n",
        "    # 2. Tokenize the text\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    # 3. Feed through the model to predict the next token probabilities\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **input_ids,\n",
        "            do_sample=True,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            temperature=temperature\n",
        "        )\n",
        "\n",
        "    # 4. Only return the answer if only_answer is True\n",
        "    output_tokens = outputs[0]\n",
        "    if only_answer:\n",
        "        output_tokens = output_tokens[input_ids['input_ids'].shape[1]:]\n",
        "\n",
        "    # 5. Decode the tokens\n",
        "    result = tokenizer.decode(output_tokens, skip_special_tokens=True)\n",
        "\n",
        "    return result\n"
      ],
      "metadata": {
        "id": "PLcswSkkLmHt"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer = chat(\n",
        "    \"What is the capital of Ireland?\",\n",
        "    only_answer=True,\n",
        "    max_new_tokens=32,\n",
        ")\n",
        "\n",
        "print(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1uBuhyvGM6D_",
        "outputId": "cb66ed7b-78a0-4f12-c637-32182d529a93"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "W0623 09:53:10.905000 6177 torch/_inductor/utils.py:1137] [0/1] Not enough SMs to use max_autotune_gemm mode\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The capital of Ireland is **Dublin**. \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LoRA is a way to finetune LLMs very efficiently by only updating a small subset of the model's parameters\n",
        "\n",
        "def apply_lora(model):\n",
        "    # Define LoRA config\n",
        "    lora_config = LoraConfig(\n",
        "        r=8, # rank of the LoRA matrices\n",
        "        task_type=\"CAUSAL_LM\",\n",
        "        target_modules=[\n",
        "            \"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    # Apply LoRA to the model\n",
        "    lora_model = get_peft_model(model, lora_config)\n",
        "    return lora_model\n",
        "\n",
        "model = apply_lora(model)\n",
        "\n",
        "# Print the number of trainable parameters after applying LoRA\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"number of trainable parameters: {trainable_params}\")\n",
        "print(f\"total parameters: {total_params}\")\n",
        "print(f\"percentage of trainable parameters: {trainable_params / total_params * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "R3Ugdc1_Narz",
        "outputId": "711d32d5-cbb1-4dc5-f76b-00783ece5755",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of trainable parameters: 10383360\n",
            "total parameters: 2624725248\n",
            "percentage of trainable parameters: 0.40%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_and_compute_loss(model, tokens, mask, context_length=512):\n",
        "    # Truncate to context length\n",
        "    tokens = tokens[:, :context_length]\n",
        "    mask = mask[:, :context_length]\n",
        "\n",
        "    # Construct the input, output, and mask\n",
        "    x = tokens[:, :-1]\n",
        "    y = tokens[:, 1:]\n",
        "    mask = mask[:, 1:]\n",
        "\n",
        "    # Forward pass to compute logits\n",
        "    logits = model(x).logits\n",
        "\n",
        "    # Compute loss\n",
        "    loss = F.cross_entropy(\n",
        "        logits.view(-1, logits.size(-1)),\n",
        "        y.view(-1),\n",
        "        reduction=\"none\"\n",
        "    )\n",
        "\n",
        "    # Mask out the loss for non-answer tokens\n",
        "    loss = loss[mask.view(-1)].mean()\n",
        "\n",
        "    return loss"
      ],
      "metadata": {
        "id": "LRmqsLI-NuNo"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Training loop ###\n",
        "\n",
        "def train(model, dataloader, tokenizer, max_steps=200, context_length=512, learning_rate=1e-4):\n",
        "    losses = []\n",
        "\n",
        "    # Apply LoRA to the model\n",
        "    # The model is already applied with LoRA before calling the train function\n",
        "    # model = apply_lora(model) # This line is no longer needed here\n",
        "\n",
        "    optimizer = Lion(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Training loop\n",
        "    for step, batch in enumerate(dataloader):\n",
        "        question = batch[\"instruction\"][0]\n",
        "        answer = batch[\"response_style\"][0]\n",
        "\n",
        "        # Format the question and answer into the template\n",
        "        text = template_with_answer.format(question=question, answer=answer) # TODO\n",
        "\n",
        "        # Tokenize the text and compute the mask for the answer\n",
        "        ids = tokenizer(text, return_tensors=\"pt\", return_offsets_mapping=True).to(model.device)\n",
        "        mask = ids[\"offset_mapping\"][:,:,0] >= text.index(answer)\n",
        "\n",
        "        # Feed the tokens through the model and compute the loss\n",
        "        loss = forward_and_compute_loss(model, ids['input_ids'], mask) # TODO\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        # monitor progress\n",
        "        if step % 10 == 0:\n",
        "            print(chat(\"What is the capital of France?\", only_answer=True))\n",
        "            print(f\"step {step} loss: {torch.mean(torch.tensor(losses)).item()}\")\n",
        "            losses = []\n",
        "\n",
        "        if step > 0 and step % max_steps == 0:\n",
        "            break\n",
        "\n",
        "    return model\n",
        "\n",
        "# Call the train function to fine-tune the model! Hint: you'll start to see results after a few dozen steps.\n",
        "# Provide the model, train_loader, and tokenizer as arguments\n",
        "model = train(model, train_loader, tokenizer) # TODO"
      ],
      "metadata": {
        "id": "WpXTPYv3NxjN",
        "outputId": "66fb30e7-c106-49d4-a914-386dd5d13306",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The capital of France is **Paris**. ðŸ‡«ðŸ‡· \n",
            "\n",
            "step 0 loss: 2.8051788806915283\n",
            "The capital of France is **Paris**. ðŸ‡«ðŸ‡· \n",
            "\n",
            "step 10 loss: 2.00321364402771\n",
            "The capital of France is **Paris**. ðŸ—¼ðŸ‡«ðŸ‡·\n",
            "step 20 loss: 1.5455166101455688\n",
            "Top o' the mornin' to ye now! Ye want to know where Paris, the capital o' France, is, do ye? Well, let\n",
            "step 30 loss: 1.6040843725204468\n",
            "Top o' the mornin' to ye! Why, it's Paris, of course, the capital o' the grand ol' France, where the\n",
            "step 40 loss: 1.5651631355285645\n",
            "Top o' the mornin' to ye, me hearty! Why, the capital of France, bless yer soul, is Paris, a fine city indeed,\n",
            "step 50 loss: 1.581621527671814\n",
            "Top o' the mornin' to ye! Now listen up, me hearty, the capital o' the grand ol' land o' France is Paris,\n",
            "step 60 loss: 1.459203839302063\n",
            "Top o' the mornin' to ye, me hearty! Ye askin' about the capital of France, did ye? Well, listen up, '\n",
            "step 70 loss: 1.4824892282485962\n",
            "Top o' the mornin' to ye, me hearty! Ye askin' about the capital o' France, ye say? Well, why, it\n",
            "step 80 loss: 1.5755536556243896\n",
            "Top o' the mornin' to ye, me hearty! Now, ye askin' about the capital, ye want to know where the bigwigs in\n",
            "step 90 loss: 1.4696494340896606\n",
            "Now, listen here ye! Top o' the mornin' to ye for askin' such a grand question! Why, the capital of France be Paris\n",
            "step 100 loss: 1.42977774143219\n",
            "Top o' the mornin' to ye now! Ye want to know about the capital of France, do ye? Well, why, the capital itself would\n",
            "step 110 loss: 1.6154733896255493\n",
            "Top o' the mornin' to ye, me hearty! Ye're askin' about the grand city of Paris, don't ye know?\n",
            "step 120 loss: 1.5842097997665405\n",
            "Top o' the mornin' to ye, me hearty! Ye ask about the capital o' France? Why, Paris, me hearty, it is!\n",
            "step 130 loss: 1.3963391780853271\n",
            "Top o' the mornin' to ye! Why, Paris, me hearty! It's as bright as a leprechaun's kiss, sure\n",
            "step 140 loss: 1.4330313205718994\n",
            "Now listen here, me hearty! The capital of France, why it's Paris, ye hear? It's a grand city, if ever there was\n",
            "step 150 loss: 1.5589344501495361\n",
            "Top o' the mornin' to ye, me hearty! Paris be the capital o' France, as grand as a castle in the clouds!\n",
            "step 160 loss: 1.5209600925445557\n",
            "Top o' the mornin' to ye! Why, the capital o' France is Paris, me hearty! It's a grand city, filled with\n",
            "step 170 loss: 1.2964279651641846\n",
            "Ah, me hearty, ye want to know about the capital o' France, do ye? Well, let me tell ye, it's Paris, me\n",
            "step 180 loss: 1.4101860523223877\n",
            "Top o' the mornin' to ye now, me hearty! Ye be askin' about the capital of France, don't ye? Well,\n",
            "step 190 loss: 1.465696096420288\n",
            "Ah, now listen here, ye curious one, ye want to know the capital of France, do ye? Well, why, let me tell ye, it\n",
            "step 200 loss: 1.4214260578155518\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(chat(\"What is a good story about tennis\", only_answer=True, max_new_tokens=200))"
      ],
      "metadata": {
        "id": "1AzW3YodV-uc",
        "outputId": "67452516-4f42-41bb-cdad-220c0c8651a7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ye be askin' about a good story about tennis, why, that's a grand question indeed! Now, let me tell ye a tale about a wee leprechaun called Paddy O'Shamrock, who was as keen on tennis as any lad could be. Now, Paddy, he wasn't just any lad, mind ye, he was a champion! He was the best there was, and he could serve a ball across the net faster than a wee leprechaun could snatch a pot o' gold.\n",
            "\n",
            "Now, one day, while Paddy was practicing his serve in the sun, he spotted a beautiful girl from the village, her hair as golden as the sun itself. Her name was Lily, and her smile could melt even the iciest heart. Now, Paddy, he was smitten, and he knew he had to win her heart.\n",
            "\n",
            "So, Paddy took a practice serve right at the edge of her field, but she was quick, and she returned it with\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(chat(\"Tell me how cognitive behavioural therapy and psychodynamic therapy are different\", only_answer=True, max_new_tokens=200))"
      ],
      "metadata": {
        "id": "oLsfTikBX2VS",
        "outputId": "381b40c8-2466-46c4-cb9b-9845208eefc5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Och, ye want to know about the differin' between cognitive behavioural therapy and psychodynamic therapy, do ye? Well, listen up, ye curious one! In cognitive behavioural therapy, the lads and lasses are all about the hows and whys, ye see. They'll be tellin' ye how to spot and change those unhelpful thought patterns and behaviours. Now, when it comes to psychodynamic therapy, they'll be lookin' at the root of the problem, ye hear? Think of it as takin' ye right back to when ye were a wee laddie or lass, tryin' to figure out why ye might be havin' such a spell of trouble.\n",
            "\n",
            "Now, in both of these therapies, they're both all about helpin' ye get through whatever's got ye in a tizzy. But the big difference is what they're tryin' to do. One's about fixin' what's wrong now,\n"
          ]
        }
      ]
    }
  ]
}